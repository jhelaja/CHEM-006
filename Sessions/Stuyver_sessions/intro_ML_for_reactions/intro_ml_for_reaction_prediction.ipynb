{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd813fc",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7daf2e9",
   "metadata": {},
   "source": [
    "The goal of this notebook is to give you a quick sense about graph neural networks for reactivity prediction. A GNN operates directly on reaction SMILES strings, and thus requires no (accurate) geometries as input. This has both upsides and downsides: on one hand, representing interacting molecules as graphs introduces an inherent loss of accuracy, as all stereochemical subtleties are lost; on the other hand, having the possibility to skip conformer generation etc. makes it possible to screen through millions of reaction possibilities at marginal computational cost.\n",
    "\n",
    "We will focus here on a model that has been trained to predict activation and reaction energies of dipolar cycloaddition reactions. Below, an overview of the model architecture can be found.\n",
    "\n",
    "Note that another environment needs to be used to run this Notebook; the environment.yml file is included in the **multitask_QM_GNN** folder.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"data/chem202300387-fig-0003-m.jpg\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfd07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('multitask_QM_GNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca767239",
   "metadata": {},
   "source": [
    "!python reactivity.py --data_path datasets/iteration0_data.csv --atom_desc_path descriptors/atom_desc_iteration0_wln.pkl --reaction_desc_path descriptors/reaction_desc_iteration0_wln.pkl --depth 2 --ini_lr 0.00165 --lr_ratio 0.93 --w_atom 0.5 --w_reaction 0.3 --hidden_size_multiplier 0 --depth_mol_ffn 1 --random_state 0 --ensemble_size 10 --splits 0 5 95 --model_dir model_iteration0 -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72388bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 14:39:49,566 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2423.52it/s]\n",
      "2025-06-08 14:39:53,722 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "2025-06-08 14:39:54.606705: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2025-06-08 14:39:54.608086: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "Model: \"wln_regressor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer (WLN_Layer)       multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention (Global_At multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "527it [01:15,  7.00it/s]                                                        \n",
      "2025-06-08 14:41:09,950 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2378.76it/s]\n",
      "2025-06-08 14:41:14,116 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "Model: \"wln_regressor_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer_1 (WLN_Layer)     multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention_1 (Global_ multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "  0%|▏                                          | 2/526 [00:01<07:47,  1.12it/s]WARNING:tensorflow:5 out of the last 528 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2a117d790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  1%|▏                                          | 3/526 [00:02<08:56,  1.03s/it]WARNING:tensorflow:6 out of the last 529 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2a117d790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "527it [01:16,  6.90it/s]                                                        \n",
      "2025-06-08 14:42:31,190 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2384.21it/s]\n",
      "2025-06-08 14:42:35,365 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "Model: \"wln_regressor_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer_2 (WLN_Layer)     multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention_2 (Global_ multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "527it [01:16,  6.93it/s]                                                        \n",
      "2025-06-08 14:43:52,070 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2372.20it/s]\n",
      "2025-06-08 14:43:56,264 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "Model: \"wln_regressor_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer_3 (WLN_Layer)     multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention_3 (Global_ multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "527it [01:17,  6.84it/s]                                                        \n",
      "2025-06-08 14:45:13,994 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2262.01it/s]\n",
      "2025-06-08 14:45:18,268 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "Model: \"wln_regressor_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer_4 (WLN_Layer)     multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention_4 (Global_ multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "527it [01:17,  6.80it/s]                                                        \n",
      "2025-06-08 14:46:36,412 - model_iteration0 - INFO - The considered atom-level descriptors are: ['partial_charge', 'fukui_elec', 'fukui_neu', 'nmr']\n",
      "postprocessing atom-wise scaling\n",
      "100%|█████████████████████████████████████| 3293/3293 [00:01<00:00, 2356.61it/s]\n",
      "2025-06-08 14:46:40,653 - model_iteration0 - INFO - The considered reaction descriptors are: ['G', 'G_alt1', 'G_alt2']\n",
      "Model: \"wln_regressor_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "wln__layer_5 (WLN_Layer)     multiple                  15000     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  6650      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  16900     \n",
      "_________________________________________________________________\n",
      "global__attention_5 (Global_ multiple                  18591     \n",
      "_________________________________________________________________\n",
      "activation_energy (Dense)    multiple                  51        \n",
      "_________________________________________________________________\n",
      "reaction_energy (Dense)      multiple                  51        \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 63,895\n",
      "Trainable params: 63,893\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      " 21%|████████▋                                | 112/526 [00:16<01:01,  6.77it/s]^C\n",
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x155248d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1263, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n",
      " 21%|████████▋                                | 112/526 [00:16<01:00,  6.84it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thijsstuyver/Desktop/teaching_resources/Stuyver_sessions/intro_ML_for_reactions/multitask_QM_GNN/reactivity.py\", line 263, in <module>\n",
      "    ) = predict_single_model(\n",
      "  File \"/Users/thijsstuyver/Desktop/teaching_resources/Stuyver_sessions/intro_ML_for_reactions/multitask_QM_GNN/utils/predict.py\", line 27, in predict_single_model\n",
      "    out = model.predict_on_batch(x)\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/keras/engine/training.py\", line 1947, in predict_on_batch\n",
      "    outputs = self.predict_function(iterator)\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\", line 917, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3039, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/opt/anaconda3/envs/qm_gnn/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python reactivity.py --data_path datasets/iteration0_data.csv --atom_desc_path descriptors/atom_desc_iteration0_wln.pkl --reaction_desc_path descriptors/reaction_desc_iteration0_wln.pkl --depth 2 --ini_lr 0.00165 --lr_ratio 0.93 --w_atom 0.5 --w_reaction 0.3 --hidden_size_multiplier 0 --depth_mol_ffn 1 --random_state 0 --ensemble_size 10 --splits 0 5 95 --model_dir model_iteration0 -p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qm_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
