{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit.Chem.Draw import MolToImage, rdMolDraw2D  # For generating images\n",
    "import base64  # For encoding images for Plotly hover\n",
    "from io import BytesIO  # For handling image data in memory\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import umap.umap_ as umap\n",
    "from dash import dcc, html, Input, Output, no_update\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "try:\n",
    "    from mordred import Calculator, descriptors as mordred_descriptors\n",
    "\n",
    "    MORDRED_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MORDRED_AVAILABLE = False\n",
    "    Calculator = None  # mordred.Calculator\n",
    "    mordred_descriptors = None  # mordred.descriptors\n",
    "\n",
    "try:\n",
    "    from openTSNE import (\n",
    "        TSNE as OpenTSNE,\n",
    "    )  # Use an alias to avoid conflict with sklearn.manifold.TSNE\n",
    "\n",
    "    OPENTSNE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENTSNE_AVAILABLE = False\n",
    "    OpenTSNE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31739aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helpers import plot_scatter_plotly_2d, plot_scatter_plotly_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load or Define Your Dataset\n",
    "# For demonstration, we'll use a small list of 100 SMILES strings from ZINC.\n",
    "# In a real scenario, you might load this from a CSV or SDF file.\n",
    "\n",
    "with open(\"SMILES_ZINC_5000.csv\", \"r\") as f:\n",
    "    smiles_list = [line.strip() for line in f.readlines()][:100]\n",
    "\n",
    "molecule_names = smiles_list # [f\"Mol_{i+1}\" for i in range(len(smiles_list))]\n",
    "\n",
    "# For later, lets create another small list of recognizable molecules\n",
    "smiles_list_extra = [\n",
    "    \"CCO\",  # Ethanol\n",
    "    \"CCC\",  # Propane\n",
    "    \"c1ccccc1\",  # Benzene\n",
    "    \"CC(=O)O\",  # Acetic acid\n",
    "    \"CNC(=O)C1=C(C=C(C=C1)N)N=NC2=CC=C(C=C2)S(=O)(=O)N\", # Prontosil\n",
    "    \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\", # Ibuprofen\n",
    "    \"COC1=CC=C(C=C1)C(C2=CC=C(C=C2)OC)O\", # Anisoin\n",
    "    \"CC1=C(C(=O)N(C1=O)C2=CC=CC=C2)C3=CC=CC=C3\", # Phenazone\n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\", # Caffeine\n",
    "    \"C1=CC=C(C=C1)C(C2=CC=CC=C2)O\", # Benzhydrol\n",
    "    \"CC(=O)OC1=CC=CC=C1C(=O)O\" # Aspirin\n",
    "]\n",
    "\n",
    "molecule_names_extra = smiles_list_extra #[f\"Mol_{i+1+len(smiles_list)}\" for i in range(len(smiles_list_extra))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate Fingerprints\n",
    "# Convert SMILES to RDKit molecule objects\n",
    "mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
    "mols_extra = [Chem.MolFromSmiles(s) for s in smiles_list_extra]\n",
    "\n",
    "# Filter out any molecules that couldn't be parsed\n",
    "valid_mols_indices = [i for i, mol in enumerate(mols) if mol is not None]\n",
    "mols = [mols[i] for i in valid_mols_indices]\n",
    "molecule_names = [molecule_names[i] for i in valid_mols_indices]\n",
    "\n",
    "if not mols:\n",
    "    raise ValueError(\"No valid molecules could be parsed from the SMILES list.\")\n",
    "\n",
    "# Using Morgan fingerprints (similar to ECFP)\n",
    "def generate_morgan_fingerprints(mol, radius=2, nBits=1024):\n",
    "    \"\"\"Generates Morgan fingerprints for a molecule.\"\"\"\n",
    "    if mol is None:\n",
    "        return np.zeros((nBits,), dtype=int)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "    return np.array(list(fp.ToBitString()), dtype=int)\n",
    "\n",
    "fingerprints = np.array([generate_morgan_fingerprints(mol) for mol in mols])\n",
    "fp_df = pd.DataFrame(fingerprints, columns=[f\"FP_{i}\" for i in range(fingerprints.shape[1])])\n",
    "fp_df.index = molecule_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577550c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate Descriptors\n",
    "# Select a few 2D descriptors\n",
    "descriptor_names = [\n",
    "    \"MolWt\", \"HeavyAtomCount\", \"NumHAcceptors\", \"NumHDonors\",\n",
    "    \"MolLogP\", \"TPSA\", \"NumRotatableBonds\", \"RingCount\"\n",
    "]\n",
    "\n",
    "def calculate_descriptors(mol, descriptor_list):\n",
    "    \"\"\"Calculates a list of RDKit descriptors for a molecule.\"\"\"\n",
    "    if mol is None:\n",
    "        return [np.nan] * len(descriptor_list)\n",
    "    vals = []\n",
    "    for desc_name in descriptor_list:\n",
    "        try:\n",
    "            # Descriptor functions are attributes of the Descriptors module\n",
    "            desc_func = getattr(Descriptors, desc_name)\n",
    "            vals.append(desc_func(mol))\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Descriptor {desc_name} not found.\")\n",
    "            vals.append(np.nan)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate {desc_name} for a molecule: {e}\")\n",
    "            vals.append(np.nan)\n",
    "    return vals\n",
    "\n",
    "descriptors_data = np.array([calculate_descriptors(mol, descriptor_names) for mol in mols])\n",
    "desc_df = pd.DataFrame(descriptors_data, columns=descriptor_names)\n",
    "desc_df.index = molecule_names\n",
    "\n",
    "descriptors_data_extra = np.array([calculate_descriptors(mol, descriptor_names) for mol in mols_extra])\n",
    "desc_df_extra = pd.DataFrame(descriptors_data_extra, columns=descriptor_names)\n",
    "desc_df_extra.index = molecule_names_extra\n",
    "\n",
    "# Handle potential NaN values from descriptor calculation (e.g., if a descriptor can't be computed)\n",
    "# A simple strategy is to fill with the mean, or you could drop rows/columns or use more sophisticated imputation.\n",
    "desc_df = desc_df.fillna(desc_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Generate MORDRED Descriptors - alternative to RDKit 2D descriptors\n",
    "if MORDRED_AVAILABLE:\n",
    "    print(\"\\nCalculating MORDRED descriptors...\")\n",
    "    # Initialize Mordred calculator. We'll ignore 3D descriptors for simplicity as we only have SMILES.\n",
    "    mordred_calc = Calculator(mordred_descriptors, ignore_3D=True)\n",
    "\n",
    "    # Calculate Mordred descriptors for all valid molecules\n",
    "    # mordred_calc.pandas() returns a DataFrame\n",
    "    mordred_desc_df_raw = mordred_calc.pandas(mols)\n",
    "\n",
    "    # Mordred might return special 'Missing' objects or exceptions as strings.\n",
    "    # Convert all columns to numeric, coercing errors to NaN.\n",
    "    mordred_desc_df = mordred_desc_df_raw.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Handle NaN values (e.g., fill with mean).\n",
    "    # First, drop columns that are all NaN (if any descriptor failed for all molecules)\n",
    "    mordred_desc_df = mordred_desc_df.dropna(axis=1, how='all')\n",
    "    # Then fill remaining NaNs with the mean of their respective column\n",
    "    mordred_desc_df = mordred_desc_df.fillna(mordred_desc_df.mean())\n",
    "    mordred_desc_df.index = molecule_names # Align index with other dataframes\n",
    "    print(f\"Calculated {mordred_desc_df.shape[1]} MORDRED descriptors.\")\n",
    "else:\n",
    "    print(\"\\nMORDRED library not found. Skipping MORDRED descriptor calculation.\")\n",
    "    # Initialize mordred_desc_df as None or an empty DataFrame if you plan to use it later\n",
    "    # For this notebook, if features_df_mordred is used, it would require mordred_desc_df to be defined.\n",
    "    # If you uncomment that line, you might want to initialize mordred_desc_df to an empty DataFrame here:\n",
    "    # mordred_desc_df = pd.DataFrame(index=molecule_names)\n",
    "    mordred_desc_df = None # Or pd.DataFrame() if you uncomment the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Combine Features if we want to\n",
    "# Feature set 1: RDKit Fingerprints + RDKit 2D Descriptors\n",
    "features_df_rdkit = pd.concat([fp_df, desc_df], axis=1)\n",
    "\n",
    "# Feature set 2: RDKit Fingerprints + MORDRED Descriptors\n",
    "if MORDRED_AVAILABLE and mordred_desc_df is not None:\n",
    "    features_df_mordred = pd.concat([fp_df, mordred_desc_df], axis=1)\n",
    "    print(\"\\nCreated features_df_mordred by combining fingerprints and MORDRED descriptors.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Preprocessing\n",
    "scaler_rdkit = StandardScaler()\n",
    "\n",
    "# We choose one set of features to work with\n",
    "scaled_features = scaler_rdkit.fit_transform(desc_df)\n",
    "\n",
    "# We can apply the scaler to unseen molecules without changing the scaler!\n",
    "scaled_features_extra = scaler_rdkit.transform(desc_df_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize with PCA\n",
    "n_components = 2\n",
    "pca_rdkit = PCA(n_components=n_components, random_state=42) # Added random_state for reproducibility\n",
    "principal_components_rdkit = pca_rdkit.fit_transform(scaled_features)\n",
    "\n",
    "pca_df_rdkit = pd.DataFrame(data=principal_components_rdkit,\n",
    "                      columns=[f'Principal Component {i+1}' for i in range(n_components)])\n",
    "pca_df_rdkit.index = molecule_names\n",
    "\n",
    "# Explained variance\n",
    "explained_variance_ratio_rdkit = pca_rdkit.explained_variance_ratio_\n",
    "print(f\"Explained variance by PCA components (RDKit Descriptors): {explained_variance_ratio_rdkit}\")\n",
    "print(f\"Total explained variance (RDKit Descriptors): {sum(explained_variance_ratio_rdkit):.2f}\")\n",
    "\n",
    "# Visualize the Chemical Space using Plotly\n",
    "fig_pca_2d = plot_scatter_plotly_2d(\n",
    "    df=pca_df_rdkit,\n",
    "    x_col='Principal Component 1',\n",
    "    y_col='Principal Component 2',\n",
    "    text_col_data=pca_df_rdkit.index, # Use index for text annotations\n",
    "    title='Chemical Space Visualization using PCA',\n",
    "    labels_dict={\n",
    "        'Principal Component 1': f'Principal Component 1 ({explained_variance_ratio_rdkit[0]:.2f} variance)',\n",
    "        'Principal Component 2': f'Principal Component 2 ({explained_variance_ratio_rdkit[1]:.2f} variance)'\n",
    "    },\n",
    "    marker_size=8,\n",
    "    text_position='top right',\n",
    ")\n",
    "fig_pca_2d.show()\n",
    "\n",
    "# Original Matplotlib/Seaborn code:\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.scatterplot(x='Principal Component 1', y='Principal Component 2', data=pca_df_rdkit, s=100, legend=False)\n",
    "# for i, name in enumerate(pca_df_rdkit.index):\n",
    "#     plt.annotate(name, (pca_df_rdkit.iloc[i, 0], pca_df_rdkit.iloc[i, 1]), textcoords=\\\"offset points\\\", xytext=(5,5), ha='left')\n",
    "# plt.title('Chemical Space Visualization using PCA')\n",
    "# plt.xlabel(f'Principal Component 1 ({explained_variance_ratio_rdkit[0]:.2f} variance)')\n",
    "# plt.ylabel(f'Principal Component 2 ({explained_variance_ratio_rdkit[1]:.2f} variance)')\n",
    "# plt.grid(True)\n",
    "# plt.axhline(0, color='grey', lw=0.5)\n",
    "# plt.axvline(0, color='grey', lw=0.5)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-component-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. Visualize with PCA (3D)\n",
    "n_components_3d = 3\n",
    "pca_3d = PCA(n_components=n_components_3d, random_state=42)\n",
    "principal_components_3d = pca_3d.fit_transform(scaled_features)\n",
    "\n",
    "pca_df_3d = pd.DataFrame(data=principal_components_3d,\n",
    "                         columns=[f'Principal Component {i+1}' for i in range(n_components_3d)])\n",
    "pca_df_3d.index = molecule_names\n",
    "\n",
    "# Explained variance\n",
    "explained_variance_ratio_3d = pca_3d.explained_variance_ratio_\n",
    "print(f\"Explained variance by 3D PCA components: {explained_variance_ratio_3d}\")\n",
    "print(f\"Total explained variance by 3D PCA: {sum(explained_variance_ratio_3d):.2f}\")\n",
    "\n",
    "# Visualize the Chemical Space in 3D using Plotly\n",
    "fig_pca_3d = plot_scatter_plotly_3d(\n",
    "    df=pca_df_3d,\n",
    "    x_col='Principal Component 1',\n",
    "    y_col='Principal Component 2',\n",
    "    z_col='Principal Component 3',\n",
    "    text_col_data=pca_df_3d.index, # Use index for text annotations\n",
    "    title='Chemical Space Visualization using 3D PCA',\n",
    "    scene_labels={ # Use scene_labels for 3D axis titles\n",
    "        'Principal Component 1': f'PC 1 ({explained_variance_ratio_3d[0]:.2f} var)',\n",
    "        'Principal Component 2': f'PC 2 ({explained_variance_ratio_3d[1]:.2f} var)',\n",
    "        'Principal Component 3': f'PC 3 ({explained_variance_ratio_3d[2]:.2f} var)'\n",
    "    },\n",
    "    marker_size=8,\n",
    ")\n",
    "fig_pca_3d.show()\n",
    "\n",
    "# Original Matplotlib code:\n",
    "# fig = plt.figure(figsize=(12, 10))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# sc = ax.scatter(pca_df_3d['Principal Component 1'],\n",
    "#                 pca_df_3d['Principal Component 2'],\n",
    "#                 pca_df_3d['Principal Component 3'],\n",
    "#                 s=100)\n",
    "# for i, name in enumerate(pca_df_3d.index):\n",
    "#     ax.text(pca_df_3d.iloc[i, 0],\n",
    "#             pca_df_3d.iloc[i, 1],\n",
    "#             pca_df_3d.iloc[i, 2],\n",
    "#             name,\n",
    "#             size=8, zorder=1, color='k')\n",
    "# ax.set_title('Chemical Space Visualization using 3D PCA')\n",
    "# ax.set_xlabel(f'PC 1 ({explained_variance_ratio_3d[0]:.2f} var)')\n",
    "# ax.set_ylabel(f'PC 2 ({explained_variance_ratio_3d[1]:.2f} var)')\n",
    "# ax.set_zlabel(f'PC 3 ({explained_variance_ratio_3d[2]:.2f} var)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-3d-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. Analyze PCA Component Loadings (Feature Contributions)\n",
    "\n",
    "# For the 2D PCA (pca_rdkit, based on desc_df)\n",
    "if 'pca_rdkit' in locals() and 'desc_df' in locals():\n",
    "    print(\"Feature contributions to 2D Principal Components (Loadings):\\n\")\n",
    "    loadings_2d_df = pd.DataFrame(\n",
    "        pca_rdkit.components_,\n",
    "        columns=desc_df.columns, # Original feature names\n",
    "        index=[f'Principal Component {i+1}' for i in range(pca_rdkit.n_components_)]\n",
    "    )\n",
    "    # Display the full loadings matrix (can be large if many features)\n",
    "    # print(loadings_2d_df.T)\n",
    "\n",
    "    # Display top N features for each component by absolute loading value\n",
    "    n_top_features = 5 \n",
    "    for pc_name in loadings_2d_df.index:\n",
    "        print(f\"\\nTop {n_top_features} features for {pc_name}:\")\n",
    "        pc_loadings = loadings_2d_df.loc[pc_name]\n",
    "        sorted_loadings = pc_loadings.abs().sort_values(ascending=False)\n",
    "        print(loadings_2d_df.loc[pc_name, sorted_loadings.index[:n_top_features]])\n",
    "else:\n",
    "    print(\"Skipping 2D PCA component analysis: pca_rdkit or desc_df not found.\")\n",
    "\n",
    "# For the 3D PCA (pca_3d, also based on desc_df via scaled_features)\n",
    "if 'pca_3d' in locals() and 'desc_df' in locals():\n",
    "    print(\"\\n\\nFeature contributions to 3D Principal Components (Loadings):\\n\")\n",
    "    loadings_3d_df = pd.DataFrame(\n",
    "        pca_3d.components_,\n",
    "        columns=desc_df.columns, # Original feature names\n",
    "        index=[f'Principal Component {i+1}' for i in range(pca_3d.n_components_)]\n",
    "    )\n",
    "    n_top_features = 5\n",
    "    for pc_name in loadings_3d_df.index:\n",
    "        print(f\"\\nTop {n_top_features} features for {pc_name}:\")\n",
    "        pc_loadings = loadings_3d_df.loc[pc_name]\n",
    "        sorted_loadings = pc_loadings.abs().sort_values(ascending=False)\n",
    "        print(loadings_3d_df.loc[pc_name, sorted_loadings.index[:n_top_features]])\n",
    "else:\n",
    "    print(\"\\nSkipping 3D PCA component analysis: pca_3d or desc_df not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25769d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Clustering with KMeans and Elbow Plot\n",
    "if scaled_features.shape[0] > 1:\n",
    "    print(\"\\nDetermining optimal number of clusters using Elbow Method for KMeans...\")\n",
    "    inertia = []\n",
    "    # Consider a range of k values. Max k should be less than number of samples.\n",
    "    max_k = min(10, scaled_features.shape[0] -1) # Max 10 clusters or n_samples-1\n",
    "    k_range = range(1, max_k + 1)\n",
    "\n",
    "    if max_k > 0:\n",
    "        for k_val in k_range:\n",
    "            kmeans = KMeans(n_clusters=k_val, random_state=42, n_init='auto')\n",
    "            kmeans.fit(scaled_features)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "\n",
    "        # Plot the elbow curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_range, inertia, marker='o')\n",
    "        plt.title('Elbow Method for Optimal k')\n",
    "        plt.xlabel('Number of clusters (k)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.xticks(k_range)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Based on the elbow plot, choose an optimal k.\n",
    "        # For this small dataset, let's assume k=3 or k=4 might be reasonable.\n",
    "        # You would typically inspect the plot to find the \"elbow\".\n",
    "        optimal_k = 5 # Example: Choose 3 clusters. Adjust based on your elbow plot.\n",
    "        if optimal_k > max_k:\n",
    "            optimal_k = max_k # Ensure optimal_k is not too large\n",
    "        \n",
    "        if optimal_k > 0:\n",
    "            print(f\"\\nPerforming KMeans clustering with k={optimal_k}...\")\n",
    "            kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "            cluster_labels_kmeans = kmeans_final.fit_predict(scaled_features)\n",
    "            print(f\"Cluster labels: {cluster_labels_kmeans}\")\n",
    "        else:\n",
    "            print(\"Skipping KMeans clustering as optimal_k is 0.\")\n",
    "            cluster_labels_kmeans = None\n",
    "    else:\n",
    "        print(\"Skipping Elbow Method: Not enough samples to form multiple clusters.\")\n",
    "        cluster_labels_kmeans = None\n",
    "else:\n",
    "    print(\"Skipping Clustering: Not enough samples.\")\n",
    "    cluster_labels_kmeans = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglomerative-clustering-7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b. Clustering with Agglomerative Clustering\n",
    "if 'scaled_features' in locals() and scaled_features.shape[0] > 1:\n",
    "    # Define the number of clusters. This can be based on domain knowledge, elbow method for other algorithms, or a fixed value.\n",
    "    # Here, we try to use optimal_k from KMeans if available and valid, otherwise a default.\n",
    "    n_clusters_agg = 0\n",
    "    if 'optimal_k' in locals() and isinstance(optimal_k, int) and optimal_k > 0 and optimal_k <= scaled_features.shape[0]:\n",
    "        n_clusters_agg = optimal_k\n",
    "        print(f\"Using optimal_k = {optimal_k} from KMeans for Agglomerative Clustering.\")\n",
    "    else:\n",
    "        # Fallback: choose a number of clusters, e.g., 3 or 5, or make it dynamic based on sample size\n",
    "        n_clusters_agg = min(5, scaled_features.shape[0]) # Default to 5 or max possible if less than 5 samples\n",
    "        if scaled_features.shape[0] < 2: n_clusters_agg = 0 # Cannot cluster if < 2 samples\n",
    "        print(f\"optimal_k from KMeans not available or invalid. Using n_clusters = {n_clusters_agg} for Agglomerative Clustering.\")\n",
    "\n",
    "    if n_clusters_agg >= 2: # AgglomerativeClustering needs n_clusters >= 2\n",
    "        print(f\"\\nPerforming Agglomerative Clustering with n_clusters={n_clusters_agg}...\")\n",
    "        agg_clustering_model = AgglomerativeClustering(n_clusters=n_clusters_agg)\n",
    "        cluster_labels_agg = agg_clustering_model.fit_predict(scaled_features)\n",
    "        print(f\"Agglomerative cluster labels (first 50): {cluster_labels_agg[:50]}\")\n",
    "    else:\n",
    "        print(\"Skipping Agglomerative Clustering: Not enough samples or n_clusters < 2.\")\n",
    "        cluster_labels_agg = None\n",
    "else:\n",
    "    print(\"\\nSkipping Agglomerative Clustering: scaled_features not available or not enough samples.\")\n",
    "    cluster_labels_agg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan-clustering-7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c. Clustering with DBSCAN\n",
    "if 'scaled_features' in locals() and scaled_features.shape[0] > 1:\n",
    "    # DBSCAN parameters: \n",
    "    # eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    # min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    # These parameters are sensitive and often require tuning based on the dataset.\n",
    "\n",
    "    # Example parameters (these will likely need adjustment for your specific dataset)\n",
    "    dbscan_eps = 0.5 # This is a critical parameter to tune.\n",
    "    if scaled_features.shape[1] > 0:\n",
    "        dbscan_min_samples = max(3, 2 * scaled_features.shape[1]) # e.g., 2 * number_of_features\n",
    "    else: \n",
    "        dbscan_min_samples = 5 \n",
    "\n",
    "    # Ensure min_samples is not greater than the number of samples and at least 1\n",
    "    if scaled_features.shape[0] < dbscan_min_samples:\n",
    "        print(f\"Warning: min_samples ({dbscan_min_samples}) is too large for the number of samples ({scaled_features.shape[0]}).\")\n",
    "        dbscan_min_samples = max(1, scaled_features.shape[0] // 10) # Adjust to a fraction or a small number\n",
    "        if scaled_features.shape[0] == 1 : dbscan_min_samples = 1\n",
    "        print(f\"Adjusted min_samples to {dbscan_min_samples}.\")\n",
    "\n",
    "    if dbscan_min_samples > 0 and scaled_features.shape[0] >= dbscan_min_samples:\n",
    "        try:\n",
    "            dbscan_model = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)\n",
    "            cluster_labels_dbscan = dbscan_model.fit_predict(scaled_features)\n",
    "            \n",
    "            n_clusters_dbscan_ = len(set(cluster_labels_dbscan)) - (1 if -1 in cluster_labels_dbscan else 0)\n",
    "            n_noise_dbscan_ = list(cluster_labels_dbscan).count(-1)\n",
    "            \n",
    "            print(f\"DBSCAN estimated number of clusters: {n_clusters_dbscan_}\")\n",
    "            print(f\"DBSCAN estimated number of noise points: {n_noise_dbscan_}\")\n",
    "            # print(f\"DBSCAN cluster labels (first 50): {cluster_labels_dbscan[:50]}\") # Can be very long\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during DBSCAN: {e}. Skipping DBSCAN.\")\n",
    "            cluster_labels_dbscan = None\n",
    "    else:\n",
    "        print(f\"Skipping DBSCAN: Not enough samples ({scaled_features.shape[0]}) for min_samples ({dbscan_min_samples}), or min_samples is 0.\")\n",
    "        cluster_labels_dbscan = None\n",
    "else:\n",
    "    print(\"\\nSkipping DBSCAN clustering: scaled_features not available or not enough samples.\")\n",
    "    cluster_labels_dbscan = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-2d-kmeans-colored",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7d. Visualize 2D PCA with cluster Labels\n",
    "if 'cluster_labels_kmeans' in locals() and cluster_labels_kmeans is not None and 'pca_df_rdkit' in locals() and 'explained_variance_ratio_rdkit' in locals():\n",
    "    pca_df_rdkit_clustered = pca_df_rdkit.copy()\n",
    "    pca_df_rdkit_clustered['Cluster'] = cluster_labels_kmeans\n",
    "    # Plotly handles categorical coloring automatically, ensure 'Cluster' column is suitable (e.g., string or category)\n",
    "    pca_df_rdkit_clustered['Cluster'] = pca_df_rdkit_clustered['Cluster'].astype(str)\n",
    "\n",
    "\n",
    "    fig_pca_2d_clustered = plot_scatter_plotly_2d(\n",
    "        df=pca_df_rdkit_clustered,\n",
    "        x_col='Principal Component 1',\n",
    "        y_col='Principal Component 2',\n",
    "        color_col='Cluster',\n",
    "        text_col_data=pca_df_rdkit_clustered.index,\n",
    "        title='2D PCA with KMeans Cluster Labels',\n",
    "        labels_dict={\n",
    "            'Principal Component 1': f'Principal Component 1 ({explained_variance_ratio_rdkit[0]:.2f} variance)',\n",
    "            'Principal Component 2': f'Principal Component 2 ({explained_variance_ratio_rdkit[1]:.2f} variance)',\n",
    "            'Cluster': 'Cluster'\n",
    "        },\n",
    "        marker_size=8,\n",
    "        text_position='top right',\n",
    "        # color_discrete_map can be used here if specific colors are desired for clusters\n",
    "    )\n",
    "    fig_pca_2d_clustered.show()\n",
    "else:\n",
    "    print(\"Skipping 2D PCA with KMeans cluster colors: Prerequisite data not available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-3d-kmeans-colored",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7e. Visualize 3D PCA with cluster Labels\n",
    "if 'cluster_labels_kmeans' in locals() and cluster_labels_kmeans is not None and 'pca_df_3d' in locals() and 'explained_variance_ratio_3d' in locals():\n",
    "    pca_df_3d_clustered = pca_df_3d.copy()\n",
    "    pca_df_3d_clustered['Cluster'] = cluster_labels_kmeans\n",
    "    # Plotly handles categorical coloring automatically, ensure 'Cluster' column is suitable\n",
    "    pca_df_3d_clustered['Cluster'] = pca_df_3d_clustered['Cluster'].astype(str)\n",
    "\n",
    "    fig_pca_3d_clustered = plot_scatter_plotly_3d(\n",
    "        df=pca_df_3d_clustered,\n",
    "        x_col='Principal Component 1',\n",
    "        y_col='Principal Component 2',\n",
    "        z_col='Principal Component 3',\n",
    "        color_col='Cluster',\n",
    "        text_col_data=pca_df_3d_clustered.index,\n",
    "        title='3D PCA with KMeans Cluster Labels',\n",
    "        scene_labels={\n",
    "            'Principal Component 1': f'PC 1 ({explained_variance_ratio_3d[0]:.2f} var)',\n",
    "            'Principal Component 2': f'PC 2 ({explained_variance_ratio_3d[1]:.2f} var)',\n",
    "            'Principal Component 3': f'PC 3 ({explained_variance_ratio_3d[2]:.2f} var)',\n",
    "            'Cluster': 'Cluster'\n",
    "        },\n",
    "        marker_size=8,\n",
    "        text_position='top right',\n",
    "        # color_discrete_map can be used here\n",
    "    )\n",
    "    fig_pca_3d_clustered.show()\n",
    "else:\n",
    "    print(\"Skipping 3D PCA with KMeans cluster colors: Prerequisite data not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6caf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Visualize with t-SNE (can sometimes give better cluster separation)\n",
    "if 'scaled_features' in locals() and scaled_features.shape[0] > 1: # t-SNE needs more than 1 sample\n",
    "    # Adjust perplexity based on your dataset size, typically 5-50\n",
    "    perplexity_value = min(30, scaled_features.shape[0] - 1)\n",
    "    if perplexity_value > 0:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value, max_iter=300, init='pca') # Added init='pca' for stability\n",
    "        tsne_results = tsne.fit_transform(scaled_features) # or scaled_features_mordred\n",
    "\n",
    "        tsne_df = pd.DataFrame(data=tsne_results,\n",
    "                               columns=['t-SNE Component 1', 't-SNE Component 2'])\n",
    "        tsne_df.index = molecule_names\n",
    "\n",
    "        fig_tsne = plot_scatter_plotly_2d(\n",
    "            df=tsne_df,\n",
    "            x_col='t-SNE Component 1',\n",
    "            y_col='t-SNE Component 2',\n",
    "            text_col_data=tsne_df.index,\n",
    "            title='Chemical Space Visualization using t-SNE',\n",
    "            labels_dict={\n",
    "                't-SNE Component 1': 't-SNE Component 1',\n",
    "                't-SNE Component 2': 't-SNE Component 2'\n",
    "            },\n",
    "            marker_size=8,\n",
    "            text_position='top right',\n",
    "            add_zerolines=False # t-SNE axes don't typically have meaning at zero\n",
    "        )\n",
    "        fig_tsne.show()\n",
    "    else:\n",
    "        print(\"Skipping t-SNE: Not enough samples for the chosen perplexity.\")\n",
    "        tsne_df = None # Ensure tsne_df is defined if t-SNE is skipped\n",
    "else:\n",
    "    print(\"Skipping t-SNE: Not enough samples or scaled_features not available.\")\n",
    "    tsne_df = None # Ensure tsne_df is defined if t-SNE is skipped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agglomerative-clustering-tsne-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8b. Cluster with Agglomerative Clustering and Color t-SNE Plot\n",
    "if 'scaled_features' in locals() and scaled_features.shape[0] > 1:\n",
    "    # Check if tsne_df was created (i.e., t-SNE ran successfully)\n",
    "    if 'tsne_df' in locals() and tsne_df is not None and not tsne_df.empty:\n",
    "        n_clusters = 0\n",
    "        if scaled_features.shape[0] > 0:\n",
    "            n_clusters = min(4, scaled_features.shape[0]) \n",
    "\n",
    "        if n_clusters >= 2: \n",
    "            agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "            cluster_labels = agg_clustering.fit_predict(scaled_features)\n",
    "\n",
    "            tsne_df_clustered = tsne_df.copy()\n",
    "            tsne_df_clustered['Cluster'] = cluster_labels\n",
    "            tsne_df_clustered['Cluster'] = tsne_df_clustered['Cluster'].astype(str) \n",
    "\n",
    "            fig_tsne_clustered = plot_scatter_plotly_2d(\n",
    "                df=tsne_df_clustered,\n",
    "                x_col='t-SNE Component 1',\n",
    "                y_col='t-SNE Component 2',\n",
    "                color_col='Cluster',\n",
    "                text_col_data=tsne_df_clustered.index,\n",
    "                title=f't-SNE Visualization with {n_clusters} Agglomerative Clusters',\n",
    "                labels_dict={\n",
    "                    't-SNE Component 1': 't-SNE Component 1',\n",
    "                    't-SNE Component 2': 't-SNE Component 2',\n",
    "                    'Cluster': 'Cluster'\n",
    "                },\n",
    "                marker_size=8,\n",
    "                text_position='top right',\n",
    "                add_zerolines=False\n",
    "            )\n",
    "            fig_tsne_clustered.show()\n",
    "        else:\n",
    "            print(\"Skipping Agglomerative Clustering: Not enough samples or clusters to form meaningful groups (need at least 2 clusters and enough samples).\")\n",
    "    elif 'tsne_df' not in locals() or tsne_df is None or tsne_df.empty:\n",
    "        print(\"Skipping Agglomerative Clustering visualization: t-SNE results (tsne_df) not available or empty.\")\n",
    "else:\n",
    "    print(\"Skipping Agglomerative Clustering: Not enough samples for t-SNE (and thus for clustering).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne-logp-affinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8c. Visualize with t-SNE using LogP difference as affinity\n",
    "if 'desc_df' in locals() and 'MolLogP' in desc_df.columns:\n",
    "    logp_values = desc_df['MolLogP'].values\n",
    "    n_samples_logp = len(logp_values)\n",
    "\n",
    "    if n_samples_logp > 1:\n",
    "        logp_distance_matrix = np.abs(logp_values[:, np.newaxis] - logp_values[np.newaxis, :])\n",
    "        perplexity_value_logp = min(30, n_samples_logp - 1)\n",
    "\n",
    "        if perplexity_value_logp > 0:\n",
    "            print(f\"\\nVisualizing with t-SNE using LogP distance matrix (perplexity={perplexity_value_logp})...\")\n",
    "            tsne_logp = TSNE(n_components=2, random_state=42, perplexity=perplexity_value_logp,\n",
    "                             metric=\"precomputed\", max_iter=300, init=\"random\") # init=\"random\" or \"pca\" if precomputed\n",
    "            tsne_logp_results = tsne_logp.fit_transform(logp_distance_matrix)\n",
    "\n",
    "            tsne_logp_df = pd.DataFrame(data=tsne_logp_results,\n",
    "                                       columns=['t-SNE LogP Component 1', 't-SNE LogP Component 2'])\n",
    "            tsne_logp_df.index = molecule_names # Assuming molecule_names is still aligned\n",
    "\n",
    "            fig_tsne_logp = plot_scatter_plotly_2d(\n",
    "                df=tsne_logp_df,\n",
    "                x_col='t-SNE LogP Component 1',\n",
    "                y_col='t-SNE LogP Component 2',\n",
    "                text_col_data=tsne_logp_df.index,\n",
    "                title='Chemical Space Visualization using t-SNE (LogP Distance)',\n",
    "                labels_dict={\n",
    "                    't-SNE LogP Component 1': 't-SNE LogP Component 1',\n",
    "                    't-SNE LogP Component 2': 't-SNE LogP Component 2'\n",
    "                },\n",
    "                marker_size=8,\n",
    "                text_position='top right',\n",
    "                add_zerolines=False\n",
    "            )\n",
    "            fig_tsne_logp.show()\n",
    "\n",
    "            # Original Matplotlib/Seaborn code:\n",
    "            # plt.figure(figsize=(12, 8))\n",
    "            # sns.scatterplot(x='t-SNE LogP Component 1', y='t-SNE LogP Component 2', data=tsne_logp_df, s=100, legend=False)\n",
    "            # for i, name in enumerate(tsne_logp_df.index):\n",
    "            #     plt.annotate(name, (tsne_logp_df.iloc[i, 0], tsne_logp_df.iloc[i, 1]), textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "            # plt.title('Chemical Space Visualization using t-SNE (LogP Distance)')\n",
    "            # plt.xlabel('t-SNE LogP Component 1')\n",
    "            # plt.ylabel('t-SNE LogP Component 2')\n",
    "            # plt.grid(True)\n",
    "            # plt.show()\n",
    "        else:\n",
    "            print(\"Skipping t-SNE with LogP distance: Not enough samples for the chosen perplexity.\")\n",
    "    else:\n",
    "        print(\"Skipping t-SNE with LogP distance: Not enough samples.\")\n",
    "else:\n",
    "    print(\"\\nSkipping t-SNE with LogP distance: MolLogP data not found in desc_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opentsne-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8d. Visualize with openTSNE (alternative t-SNE implementation)\n",
    "if OPENTSNE_AVAILABLE:\n",
    "    print(\"\\nVisualizing with openTSNE...\")\n",
    "    if 'scaled_features' in locals() and scaled_features.shape[0] > 1:\n",
    "        perplexity_value_otsne = min(30, scaled_features.shape[0] - 1)\n",
    "        if perplexity_value_otsne > 0:\n",
    "            otsne = OpenTSNE(n_components=2, perplexity=perplexity_value_otsne, random_state=42, n_jobs=-1, n_iter=300)\n",
    "            otsne_embedding = otsne.fit(scaled_features) # openTSNE's fit returns the embedding\n",
    "\n",
    "            otsne_df = pd.DataFrame(data=otsne_embedding,\n",
    "                                   columns=['openTSNE Component 1', 'openTSNE Component 2'])\n",
    "            otsne_df.index = molecule_names # Assuming molecule_names is aligned\n",
    "\n",
    "            fig_otsne = plot_scatter_plotly_2d(\n",
    "                df=otsne_df,\n",
    "                x_col='openTSNE Component 1',\n",
    "                y_col='openTSNE Component 2',\n",
    "                text_col_data=otsne_df.index,\n",
    "                title='Chemical Space Visualization using openTSNE',\n",
    "                labels_dict={\n",
    "                    'openTSNE Component 1': 'openTSNE Component 1',\n",
    "                    'openTSNE Component 2': 'openTSNE Component 2'\n",
    "                },\n",
    "                marker_size=8,\n",
    "                text_position='top right',\n",
    "                add_zerolines=False\n",
    "            )\n",
    "            fig_otsne.show()\n",
    "\n",
    "            # Original Matplotlib/Seaborn code:\n",
    "            # plt.figure(figsize=(12, 8))\n",
    "            # sns.scatterplot(x='openTSNE Component 1', y='openTSNE Component 2', data=otsne_df, s=100, legend=False)\n",
    "            # for i, name in enumerate(otsne_df.index):\n",
    "            #     plt.annotate(name, (otsne_df.iloc[i, 0], otsne_df.iloc[i, 1]), textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "            # plt.title('Chemical Space Visualization using openTSNE')\n",
    "            # plt.xlabel('openTSNE Component 1')\n",
    "            # plt.ylabel('openTSNE Component 2')\n",
    "            # plt.grid(True)\n",
    "            # plt.show()\n",
    "        else:\n",
    "            print(\"Skipping openTSNE: Not enough samples for the chosen perplexity.\")\n",
    "    else:\n",
    "        print(\"Skipping openTSNE: Not enough samples or scaled_features not available.\")\n",
    "else:\n",
    "    print(\"\\nopenTSNE library not found. Skipping openTSNE visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Visualize with UMAP\n",
    "if 'scaled_features' in locals() and scaled_features.shape[0] > 1: # UMAP also needs more than 1 sample\n",
    "    n_neighbors_value = min(15, scaled_features.shape[0] - 1)\n",
    "    if n_neighbors_value > 1: # UMAP n_neighbors must be > 1\n",
    "        reducer = umap.UMAP(n_neighbors=n_neighbors_value, n_components=2, min_dist=0.1, random_state=42)\n",
    "        umap_results = reducer.fit_transform(scaled_features)\n",
    "\n",
    "        umap_df = pd.DataFrame(data=umap_results,\n",
    "                               columns=['UMAP Component 1', 'UMAP Component 2'])\n",
    "        umap_df.index = molecule_names # Assuming molecule_names is aligned\n",
    "\n",
    "        fig_umap = plot_scatter_plotly_2d(\n",
    "            df=umap_df,\n",
    "            x_col='UMAP Component 1',\n",
    "            y_col='UMAP Component 2',\n",
    "            text_col_data=umap_df.index,\n",
    "            title='Chemical Space Visualization using UMAP',\n",
    "            labels_dict={\n",
    "                'UMAP Component 1': 'UMAP Component 1',\n",
    "                'UMAP Component 2': 'UMAP Component 2'\n",
    "            },\n",
    "            marker_size=8,\n",
    "            text_position='top right',\n",
    "            add_zerolines=False # UMAP axes don't typically have meaning at zero\n",
    "        )\n",
    "        fig_umap.show()\n",
    "\n",
    "        # Original Matplotlib/Seaborn code:\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # sns.scatterplot(x='UMAP Component 1', y='UMAP Component 2', data=umap_df, s=100, legend=False)\n",
    "        # for i, name in enumerate(umap_df.index):\n",
    "        #     plt.annotate(name, (umap_df.iloc[i, 0], umap_df.iloc[i, 1]), textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "        # plt.title('Chemical Space Visualization using UMAP')\n",
    "        # plt.xlabel('UMAP Component 1')\n",
    "        # plt.ylabel('UMAP Component 2')\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "    else:\n",
    "        print(\"Skipping UMAP: Not enough samples for the chosen n_neighbors.\")\n",
    "else:\n",
    "    print(\"Skipping UMAP: Not enough samples or scaled_features not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe226d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try fetching some larger database \n",
    "# (e.g. Data_FORMED.csv from https://archive.materialscloud.org/record/2024.104 or more ZINC molecules)\n",
    "# and play with the visualization parameters and options above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7137824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try plotting a few complex drug molecules using an embedding fit on ZINC. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a final diagnostic, one can consider whether the separation between\n",
    "# points in the embedding is correlated to the separation between the properties of interest.\n",
    "# Can you give this a go? :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c2148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c128a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77ce80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8c744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cc5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c147b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09573ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab676a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb3179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b737d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb504f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f16343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5021d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d564980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
